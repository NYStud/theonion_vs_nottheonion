{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI SVD\n",
    "See https://medium.com/data-from-the-trenches/arithmetic-properties-of-word-embeddings-e918e3fda2ac\n",
    "\n",
    "and\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "\n",
    "and \n",
    "\n",
    "https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:10:02.733312Z",
     "start_time": "2020-01-30T20:09:59.515830Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:10:06.800641Z",
     "start_time": "2020-01-30T20:10:05.426313Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer,\\\n",
    "                                            TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:30:50.997839Z",
     "start_time": "2020-01-30T20:30:50.992852Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:10:14.385990Z",
     "start_time": "2020-01-30T20:10:14.378009Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, filename):\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(filename):\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:10:19.131555Z",
     "start_time": "2020-01-30T20:10:19.127566Z"
    }
   },
   "outputs": [],
   "source": [
    "DIR = \"C:\\\\Users\\\\AzNsAnTaGiN\\\\DSI\\\\Projects\\\\project_3\\\\data\\\\\"\n",
    "FILE1 = \"theonion\"\n",
    "FILE2 = \"nottheonion\"\n",
    "FILE3 = \"onionheadlines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:12:52.103234Z",
     "start_time": "2020-01-30T20:12:51.876788Z"
    }
   },
   "outputs": [],
   "source": [
    "X_theonion = load_obj(DIR+FILE1+\"_df_clean\")\n",
    "X_nottheonion = load_obj(DIR+FILE2+\"_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:12:52.716072Z",
     "start_time": "2020-01-30T20:12:52.706097Z"
    }
   },
   "outputs": [],
   "source": [
    "X_theonion[\"is_onion\"] = 1\n",
    "X_nottheonion[\"is_onion\"] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing with `spacy`\n",
    "\n",
    "We follow along with the tutorial found here\n",
    "https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T22:49:16.026410Z",
     "start_time": "2020-01-30T22:49:15.831850Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T22:50:29.162152Z",
     "start_time": "2020-01-30T22:50:29.149186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kidnapped\n",
      "Teen\n",
      "Freed\n",
      ",\n",
      "Though\n",
      "Freedom\n",
      "Is\n",
      "Its\n",
      "Own\n",
      "Kind\n",
      "Of\n",
      "Prison\n",
      ",\n",
      "Is\n",
      "It\n",
      "Not\n",
      "?\n",
      "Local\n",
      "Teen\n",
      "Walks\n",
      "In\n",
      "On\n",
      "Family\n",
      "Masturbating\n",
      "Nelson\n",
      "Mandela\n",
      "Becomes\n",
      "First\n",
      "Politician\n",
      "To\n",
      "Be\n",
      "Missed\n",
      "Grisly\n",
      "Remains\n",
      "Of\n",
      "15\n",
      "Hobbits\n",
      "Discovered\n",
      "In\n",
      "Peter\n",
      "Jacksons\n",
      "Attic\n",
      "New\n",
      "Documentary\n",
      "Reveals\n",
      "SeaWorld\n",
      "Forced\n",
      "Orca\n",
      "Whales\n",
      "To\n",
      "Perform\n",
      "Nude\n"
     ]
    }
   ],
   "source": [
    "for i in X_theonion[\"title\"][5:10].values:\n",
    "#     for j in sp(i):\n",
    "#         print(j)\n",
    "    for token in nlp(i):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spacy` implements entity detection, i.e. it is capable of discerning that `Manchester United` should be tokenized as one entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T22:45:08.330124Z",
     "start_time": "2020-01-30T22:45:07.745608Z"
    }
   },
   "outputs": [],
   "source": [
    "# # from stackedoverflow:\n",
    "# # https://stackoverflow.com/questions/54640715/tokenizing-named-entities-in-spacy\n",
    "# class EntityRetokenizeComponent:\n",
    "#   def __init__(self, nlp):\n",
    "#     pass\n",
    "#   def __call__(self, doc):\n",
    "#     with doc.retokenize() as retokenizer:\n",
    "#         for ent in doc.ents:\n",
    "#             retokenizer.merge(doc[ent.start:ent.end], attrs={\"LEMMA\": str(doc[ent.start:ent.end])})\n",
    "#     return doc\n",
    "\n",
    "# sp = spacy.load(\"en_core_web_sm\")\n",
    "# retokenizer = EntityRetokenizeComponent(sp) \n",
    "# sp.add_pipe(retokenizer, name='merge_phrases', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T22:45:32.843659Z",
     "start_time": "2020-01-30T22:45:32.821718Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-50cd26981ae9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m           \"converse in the Oval Office inside the White House in Washington, D.C.\")\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "# doc = sp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "#           \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "\n",
    "# doc.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:40:31.728744Z",
     "start_time": "2020-01-30T20:40:31.681870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nelson Mandela\n",
      "First\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in X_theonion[\"title\"][5:10].values:\n",
    "#     for j in sp(i):\n",
    "#         print(j)\n",
    "    for entity in sp(i).ents:\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-30T20:43:18.096128Z",
     "start_time": "2020-01-30T20:43:18.041275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Kidnapped, Teen, Freed, ,, Though, Freedom, Is, Its, Own, Kind, Of, Prison, ,, Is, It, Not, ?]\n",
      "[Local, Teen, Walks, In, On, Family, Masturbating]\n",
      "[Nelson, Mandela, Becomes, First, Politician, To, Be, Missed]\n",
      "[Grisly, Remains, Of, 15, Hobbits, Discovered, In, Peter, Jacksons, Attic]\n",
      "[New, Documentary, Reveals, SeaWorld, Forced, Orca, Whales, To, Perform, Nude]\n"
     ]
    }
   ],
   "source": [
    "for i in X_theonion[\"title\"][5:10].values:\n",
    "    print([token for token in sp(i)])\n",
    "#     for entity in sp(i).ents:\n",
    "#         print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
