{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI SVD\n",
    "See https://medium.com/data-from-the-trenches/arithmetic-properties-of-word-embeddings-e918e3fda2ac\n",
    "\n",
    "and\n",
    "\n",
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "\n",
    "and \n",
    "\n",
    "https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:18:03.043834Z",
     "start_time": "2020-01-31T12:18:02.220525Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from string import punctuation\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:18:18.943744Z",
     "start_time": "2020-01-31T12:18:18.939743Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:18:06.615503Z",
     "start_time": "2020-01-31T12:18:06.612509Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "# # Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "# from sklearn.feature_extraction.text import CountVectorizer,\\\n",
    "#                                             TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:18:08.373551Z",
     "start_time": "2020-01-31T12:18:08.369549Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:18:20.337043Z",
     "start_time": "2020-01-31T12:18:20.331060Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, filename):\n",
    "    with open(filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(filename):\n",
    "    with open(filename + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:18:23.877908Z",
     "start_time": "2020-01-31T12:18:23.872510Z"
    }
   },
   "outputs": [],
   "source": [
    "DIR = \"C:\\\\Users\\\\AzNsAnTaGiN\\\\DSI\\\\Projects\\\\project_3\\\\data\\\\\"\n",
    "FILE1 = \"theonion\"\n",
    "FILE2 = \"nottheonion\"\n",
    "FILE3 = \"onionheadlines\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:20:34.320752Z",
     "start_time": "2020-01-31T12:20:34.115547Z"
    }
   },
   "outputs": [],
   "source": [
    "X_theonion = load_obj(DIR+FILE1+\"_df_clean\")\n",
    "X_nottheonion = load_obj(DIR+FILE2+\"_df_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:20:35.116650Z",
     "start_time": "2020-01-31T12:20:35.108660Z"
    }
   },
   "outputs": [],
   "source": [
    "X_theonion[\"is_onion\"] = 1\n",
    "X_nottheonion[\"is_onion\"] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating our samples and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:20:36.216336Z",
     "start_time": "2020-01-31T12:20:36.050168Z"
    }
   },
   "outputs": [],
   "source": [
    "N=4000\n",
    "X_theonion_shuffled = X_theonion.sample(len(X_theonion))\n",
    "theonion_sample = X_theonion_shuffled.head(N)\n",
    "theonion_holdout = X_theonion_shuffled.tail(len(X_theonion_shuffled) - N)\n",
    "\n",
    "X_nottheonion_shuffled = X_nottheonion.sample(len(X_nottheonion))\n",
    "nottheonion_sample = X_nottheonion_shuffled.head(N)\n",
    "nottheonion_holdout = X_nottheonion_shuffled.tail(len(X_nottheonion_shuffled)-N)\n",
    "X_sample = pd.concat([theonion_sample, nottheonion_sample])\n",
    "X = pd.concat([X_theonion, X_nottheonion])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the PPMI sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code we use in section to build the PPMI SVD is lifted virtually verbatim from https://www.kaggle.com/alexklibisz/simple-word-vectors-with-co-occurrence-pmi-and-svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:20:38.040673Z",
     "start_time": "2020-01-31T12:20:38.033712Z"
    }
   },
   "outputs": [],
   "source": [
    "punctrans = str.maketrans(dict.fromkeys(punctuation))\n",
    "def tokenize(title):\n",
    "    x = title.lower() # Lowercase\n",
    "    x = x.encode('ascii', 'ignore').decode() # Keep only ascii chars.\n",
    "    x = x.translate(punctrans) # Remove punctuation\n",
    "    return x.split() # Return tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:22:12.292837Z",
     "start_time": "2020-01-31T12:22:10.042173Z"
    }
   },
   "outputs": [],
   "source": [
    "texts_tokenized = X['title'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:23:01.853845Z",
     "start_time": "2020-01-31T12:22:27.466094Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2a. Compute unigram and bigram counts.\n",
    "# A unigram is a single word (x). A bigram is a pair of words (x,y).\n",
    "# Bigrams are counted for any two terms occurring in the same title.\n",
    "# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n",
    "# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\n",
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "for text in texts_tokenized:\n",
    "    \n",
    "    for x in text:\n",
    "        cx[x] += 1\n",
    "\n",
    "    # Count all pairs of words, even duplicate pairs.\n",
    "    for x, y in map(sorted, combinations(text, 2)):\n",
    "        cxy[(x, y)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:23:16.217186Z",
     "start_time": "2020-01-31T12:23:16.126345Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2b. Remove frequent and infrequent unigrams.\n",
    "# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n",
    "# very frequently or infrequently. This decreases the vocab size substantially.\n",
    "min_count = (1 / 1000) * len(X)\n",
    "max_count = (1 / 50) * len(X)\n",
    "for x in list(cx.keys()):\n",
    "    if cx[x] < min_count or cx[x] > max_count:\n",
    "        del cx[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:23:44.311993Z",
     "start_time": "2020-01-31T12:23:36.974553Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2c. Remove frequent and infrequent bigrams.\n",
    "# Any bigram containing a unigram that was removed must now be removed.\n",
    "for x, y in list(cxy.keys()):\n",
    "    if x not in cx or y not in cx:\n",
    "        del cxy[(x, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:23:49.452314Z",
     "start_time": "2020-01-31T12:23:49.444333Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Build unigram <-> index lookup.\n",
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:24:05.859300Z",
     "start_time": "2020-01-31T12:24:05.816413Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Sum unigram and bigram counts for computing probabilities.\n",
    "# i.e. p(x) = count(x) / sum(all counts).\n",
    "\n",
    "sx = sum(cx.values())\n",
    "sxy = sum(cxy.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:37:36.529737Z",
     "start_time": "2020-01-31T12:37:33.665123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.244 seconds (0.00000 / iter)\n",
      "858472 non-zero elements\n",
      "Sample PMI values\n",
      " [(('hires', 'hitman'), 8.140560333991958),\n",
      " (('hitman', 'hitman'), 7.858863688555038),\n",
      " (('hong', 'kong'), 7.482190770366894),\n",
      " (('johns', 'papa'), 7.298524531355853),\n",
      " (('nobel', 'prize'), 6.984464302392243),\n",
      " (('elon', 'musk'), 6.931772124585697),\n",
      " (('ben', 'carson'), 6.8461965050086535),\n",
      " (('bieber', 'justin'), 6.808720346409335),\n",
      " (('hires', 'hires'), 6.786321554316943),\n",
      " (('bell', 'taco'), 6.632516281195759),\n",
      " (('biden', 'joe'), 6.6122186891864105),\n",
      " (('murdering', 'wrote'), 6.578223357062715),\n",
      " (('francisco', 'san'), 6.53800000840054),\n",
      " (('bernie', 'sanders'), 6.502903933290499),\n",
      " (('francis', 'pope'), 6.461349157531503),\n",
      " (('nobel', 'peace'), 6.4516270820200905),\n",
      " (('posing', 'undercover'), 6.389303431446813),\n",
      " (('cruz', 'ted'), 6.372277364180921),\n",
      " (('card', 'credit'), 6.353846318345864),\n",
      " (('parade', 'pride'), 6.3415614542873655),\n",
      " (('lives', 'matter'), 6.329011988445763),\n",
      " (('assaulted', 'sexually'), 6.325520951755993),\n",
      " (('bowl', 'super'), 6.316425536296509),\n",
      " (('jong', 'un'), 6.283940702814538),\n",
      " (('guilty', 'pleads'), 6.279186229984436),\n",
      " (('earth', 'flat'), 6.268072116511513),\n",
      " (('putin', 'vladimir'), 6.219390715908419),\n",
      " (('lot', 'parking'), 6.216275167264667),\n",
      " (('ford', 'rob'), 6.2088943980592735),\n",
      " (('husband', 'murdering'), 6.177453534132225)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n",
    "# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n",
    "# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n",
    "# The probabilities are computed on the fly using the sums from above.\n",
    "t0 = time()\n",
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in cxy.items():\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    data.append(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)))\n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PMI = csc_matrix((data, (rows, cols)))\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\n",
    "print('%d non-zero elements' % PMI.count_nonzero())\n",
    "print('Sample PMI values\\n', pformat(pmi_samples.most_common()[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:25:23.753356Z",
     "start_time": "2020-01-31T12:25:23.453039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.295 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. Factorize the PMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n",
    "# This part replaces the stochastic gradient descent used by Word2vec\n",
    "# and other related neural network formulations. We pick an arbitrary vector size k=20.\n",
    "t0 = time()\n",
    "U, _, _ = svds(PMI, k=20)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:25:30.900546Z",
     "start_time": "2020-01-31T12:25:30.894562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 seconds\n"
     ]
    }
   ],
   "source": [
    "# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n",
    "# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\n",
    "t0 = time()\n",
    "norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\n",
    "U /= np.maximum(norms, 1e-7)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:25:38.706434Z",
     "start_time": "2020-01-31T12:25:38.685503Z"
    }
   },
   "source": [
    "If we wanted to use this for modelling, we'd ideally implement a `fit()` and `transform()` method so we could toss it into a pipeline. Unfortunately, we will have to defer that to a future date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
